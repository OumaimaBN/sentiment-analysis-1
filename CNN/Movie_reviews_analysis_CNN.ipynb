{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie-reviews-analysis-CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "EDdXIWs3ys_h",
        "outputId": "1c570162-a36e-489d-8b25-56eedb037418"
      },
      "source": [
        "from numpy import array\r\n",
        "from string import punctuation\r\n",
        "from os import listdir\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "import numpy as np\r\n",
        "from collections import Counter\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Dropout\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "%cd /content/drive/My Drive/TP1-NLP\r\n",
        "# load doc into memory\r\n",
        "def load_doc(filename):\r\n",
        "\t# open the file as read only\r\n",
        "\tfile = open(filename, 'r')\r\n",
        "\t# read all text\r\n",
        "\ttext = file.read()\r\n",
        "\t# close the file\r\n",
        "\tfile.close()\r\n",
        "\treturn text\r\n",
        "\r\n",
        "# turn a doc into clean tokens\r\n",
        "def clean_doc(doc):\r\n",
        "\t# split into tokens by white space\r\n",
        "\ttokens = doc.split()\r\n",
        "\t# remove punctuation from each token\r\n",
        "\ttable = str.maketrans('', '', punctuation)\r\n",
        "\ttokens = [w.translate(table) for w in tokens]\r\n",
        "\t# remove remaining tokens that are not alphabetic\r\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\r\n",
        "\t# filter out stop words\r\n",
        "\tstop_words = set(stopwords.words('english'))\r\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\r\n",
        "\t# filter out short tokens\r\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\r\n",
        "\treturn tokens\r\n",
        "\r\n",
        "# load doc and add to vocab\r\n",
        "def add_doc_to_vocab(filename, vocab):\r\n",
        "\t# load doc\r\n",
        "\tdoc = load_doc(filename)\r\n",
        "\t# clean doc\r\n",
        "\ttokens = clean_doc(doc)\r\n",
        "\t# update counts\r\n",
        "\tvocab.update(tokens)\r\n",
        "\r\n",
        "# load all docs in a directory\r\n",
        "def process_docs(directory, vocab):\r\n",
        "\t# walk through all files in the folder\r\n",
        "\tfor filename in listdir(directory):\r\n",
        "\t\t# skip any reviews in the test set\r\n",
        "\t\tif filename.startswith('cv9'):\r\n",
        "\t\t\tcontinue\r\n",
        "\t\t# create the full path of the file to open\r\n",
        "\t\tpath = directory + '/' + filename\r\n",
        "\t\t# add doc to vocab\r\n",
        "\t\tadd_doc_to_vocab(path, vocab)\r\n",
        "\r\n",
        "# define vocab\r\n",
        "vocab = Counter()\r\n",
        "# add all docs to vocab\r\n",
        "process_docs('txt_sentoken/pos', vocab)\r\n",
        "process_docs('txt_sentoken/neg', vocab)\r\n",
        "# print the size of the vocab\r\n",
        "print(len(vocab))\r\n",
        "# print the top words in the vocab\r\n",
        "print(vocab.most_common(50))\r\n",
        "\r\n",
        "# save list to file\r\n",
        "def save_list(lines, filename):\r\n",
        "\t# convert lines to a single blob of text\r\n",
        "\tdata = '\\n'.join(lines)\r\n",
        "\t# open file\r\n",
        "\tfile = open(filename, 'w')\r\n",
        "\t# write text\r\n",
        "\tfile.write(data)\r\n",
        "\t# close file\r\n",
        "\tfile.close()\r\n",
        "\r\n",
        "# keep tokens with a min occurrence\r\n",
        "min_occurane = 2\r\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\r\n",
        "print(len(tokens))\r\n",
        "# save tokens to a vocabulary file\r\n",
        "save_list(tokens, 'vocab1.txt')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/TP1-NLP\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-bc416285a315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# add all docs to vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'txt_sentoken/pos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'txt_sentoken/neg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;31m# print the size of the vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-bc416285a315>\u001b[0m in \u001b[0;36mprocess_docs\u001b[0;34m(directory, vocab)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;31m# add doc to vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0madd_doc_to_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# define vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-bc416285a315>\u001b[0m in \u001b[0;36madd_doc_to_vocab\u001b[0;34m(filename, vocab)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_doc_to_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# load doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;31m# clean doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-bc416285a315>\u001b[0m in \u001b[0;36mload_doc\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# read all text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# close the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d8mqaE8zAe0",
        "outputId": "c5dedebb-052e-4706-88ec-68a81cd9a148"
      },
      "source": [
        "from numpy import array\r\n",
        "from string import punctuation\r\n",
        "from os import listdir\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "import numpy as np\r\n",
        "from collections import Counter\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZdy_XALZGzI"
      },
      "source": [
        "    \r\n",
        "\r\n",
        "# turn a doc into clean tokens\r\n",
        "def clean_doc(doc):\r\n",
        "\t# split into tokens by white space\r\n",
        "\ttokens = doc.split()\r\n",
        "\t# remove punctuation from each token\r\n",
        "\ttable = str.maketrans('', '', punctuation)\r\n",
        "\ttokens = [w.translate(table) for w in tokens]\r\n",
        "\t# remove remaining tokens that are not alphabetic\r\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\r\n",
        "\t# filter out stop words\r\n",
        "\tstop_words = set(stopwords.words('english'))\r\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\r\n",
        "\t# filter out short tokens\r\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\r\n",
        "\treturn tokens\r\n",
        "\r\n",
        "# load doc, clean and return line of tokens\r\n",
        "def doc_to_line(filename, vocab):\r\n",
        "\t# load the doc\r\n",
        "\tdoc = load_doc(filename)\r\n",
        "\t# clean doc\r\n",
        "\ttokens = clean_doc(doc)\r\n",
        "\t# filter by vocab\r\n",
        "\ttokens = [w for w in tokens if w in vocab]\r\n",
        "\treturn ' '.join(tokens)\r\n",
        "\r\n",
        "# load all docs in a directory\r\n",
        "def process_docs(directory, vocab):\r\n",
        "\tlines = list()\r\n",
        "\t# walk through all files in the folder\r\n",
        "\tfor filename in listdir(directory):\r\n",
        "\t\t# skip any reviews in the test set\r\n",
        "\t\t'''if filename.startswith('cv9'):\r\n",
        "\t\t\tcontinue'''\r\n",
        "\t\t# create the full path of the file to open\r\n",
        "\t\tpath = directory + '/' + filename\r\n",
        "\t\t# load and clean the doc\r\n",
        "\t\tline = doc_to_line(path, vocab)\r\n",
        "\t\t# add to list\r\n",
        "\t\tlines.append(line)\r\n",
        "\treturn lines\r\n",
        "\r\n",
        "# load the vocabulary\r\n",
        "vocab_filename = 'vocab1.txt'\r\n",
        "vocab = load_doc(vocab_filename)\r\n",
        "vocab = vocab.split()\r\n",
        "vocab = set(vocab)\r\n",
        "# load all training reviews\r\n",
        "def load_clean_dataset(vocab):\r\n",
        "  pos_labels = list()\r\n",
        "  neg_labels = list()\r\n",
        "  # load all training reviews\r\n",
        "  positive_lines = process_docs('txt_sentoken/pos', vocab)# print(type(positive_lines))\r\n",
        "  for i in range(len(positive_lines)):pos_labels.append(1)\r\n",
        "  negative_lines = process_docs('txt_sentoken/neg', vocab)\r\n",
        "  for i in range(len(negative_lines)):neg_labels.append(0)\r\n",
        "  docs = positive_lines + negative_lines\r\n",
        "  labels = np.array(pos_labels + neg_labels)\r\n",
        "  return docs, labels\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp0U1FAy9Vdy"
      },
      "source": [
        "# fit a tokenizer\r\n",
        "def create_tokenizer(lines):\r\n",
        "    tokenizer = Tokenizer()\r\n",
        "    tokenizer.fit_on_texts(lines)\r\n",
        "    return tokenizer\r\n",
        "# integer encode and pad documents\r\n",
        "def encode_docs(tokenizer, max_length, documents):\r\n",
        "    # integer encode\r\n",
        "    encoded = tokenizer.texts_to_sequences(documents)\r\n",
        "    # pad sequence\r\n",
        "    padded = pad_sequences(encoded, maxlen=max_length, \r\n",
        "             padding='post')\r\n",
        "    return padded\r\n",
        "\r\n",
        "# create the model\r\n",
        "def create_model(vocabulary_size, max_length):\r\n",
        "    # define network\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Embedding(vocabulary_size, 100, \r\n",
        "               input_length=max_length))\r\n",
        "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\r\n",
        "    model.add(MaxPooling1D(pool_size=2))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(10, activation='relu'))\r\n",
        "    model.add(Dense(1, activation='sigmoid'))\r\n",
        "    # compile network\r\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', \r\n",
        "                  metrics=['accuracy'])\r\n",
        "    # summarize defined model\r\n",
        "    model.summary()\r\n",
        "    return model\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvW1oJHdCgI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9262e5f7-7cea-4e2b-d0d1-cc3502931254"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import Embedding\r\n",
        "from keras.layers.convolutional import Conv1D\r\n",
        "from keras.layers.convolutional import MaxPooling1D\r\n",
        "from math import floor\r\n",
        "from random import shuffle\r\n",
        "import re\r\n",
        "# load all reviews\r\n",
        "docs, labels = load_clean_dataset(vocab)\r\n",
        "# create the tokenizer\r\n",
        "tokenizer = create_tokenizer(docs)\r\n",
        "# split data for training and testing\r\n",
        "split_limit = 1800\r\n",
        "train_docs, ytrain = docs[:split_limit], labels[:split_limit]\r\n",
        "test_docs, ytest = docs[split_limit:], labels[split_limit:]\r\n",
        "# calculate maximum sequence length\r\n",
        "max_length = max([len(s.split()) for s in docs])\r\n",
        "# encode data\r\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\r\n",
        "Xtest = encode_docs(tokenizer, max_length, test_docs)\r\n",
        "print(Xtrain.shape, Xtest.shape)\r\n",
        "n_words = Xtest.shape[1]\r\n",
        "# Vocabulary size\r\n",
        "vocabulary_size = len(tokenizer.word_index)+1\r\n",
        "# compile network\r\n",
        "\r\n",
        "model = create_model(vocabulary_size, max_length)\r\n",
        "\r\n",
        "# fit network\r\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\r\n",
        "# evaluate\r\n",
        "#ytest = np.array(ytest)\r\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\r\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1800, 1317) (200, 1317)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1317, 100)         2576800   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 1310, 32)          25632     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 655, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 20960)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                209610    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 2,812,053\n",
            "Trainable params: 2,812,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "57/57 - 16s - loss: 0.6796 - accuracy: 0.5606\n",
            "Epoch 2/10\n",
            "57/57 - 15s - loss: 0.5231 - accuracy: 0.6850\n",
            "Epoch 3/10\n",
            "57/57 - 15s - loss: 0.3318 - accuracy: 0.9644\n",
            "Epoch 4/10\n",
            "57/57 - 15s - loss: 0.2840 - accuracy: 0.9906\n",
            "Epoch 5/10\n",
            "57/57 - 15s - loss: 0.2653 - accuracy: 0.9972\n",
            "Epoch 6/10\n",
            "57/57 - 15s - loss: 0.2522 - accuracy: 0.9989\n",
            "Epoch 7/10\n",
            "57/57 - 15s - loss: 0.2408 - accuracy: 0.9989\n",
            "Epoch 8/10\n",
            "57/57 - 15s - loss: 0.2304 - accuracy: 0.9989\n",
            "Epoch 9/10\n",
            "57/57 - 15s - loss: 0.2206 - accuracy: 0.9989\n",
            "Epoch 10/10\n",
            "57/57 - 15s - loss: 0.2114 - accuracy: 0.9989\n",
            "Test Accuracy: 54.000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqMDQ-I9qoV7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL5I6aS1J7Db"
      },
      "source": [
        "# Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8JqFFWVJ8B9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f8516b-ad17-49a2-e835-574babe98254"
      },
      "source": [
        "# load the whole embedding into memory\r\n",
        "embeddings_index = dict()\r\n",
        "f = open('glove.6B.100d.txt')\r\n",
        "for line in f:\r\n",
        " values = line.split()\r\n",
        " word = values[0]\r\n",
        " coefs = np.asarray(values[1:], dtype='float32')\r\n",
        " embeddings_index[word] = coefs\r\n",
        "f.close()\r\n",
        "print('Loaded %s word vectors.' % len(embeddings_index)) \r\n",
        "# create a weight matrix for words in training docs\r\n",
        "embedding_matrix = np.zeros((vocabulary_size, 100))\r\n",
        "for word, i in tokenizer.word_index.items():\r\n",
        " embedding_vector = embeddings_index.get(word)\r\n",
        " if embedding_vector is not None:\r\n",
        "   embedding_matrix[i] = embedding_vector \r\n",
        "\r\n",
        "# create the model\r\n",
        "def create_model(vocabulary_size, max_length, weights):\r\n",
        "    # define network\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Embedding(vocabulary_size, 100, weights = [weights],input_length=max_length, trainable=False))\r\n",
        "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\r\n",
        "    model.add(MaxPooling1D(pool_size=2))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(10, activation='relu'))\r\n",
        "    model.add(Dense(1, activation='sigmoid'))\r\n",
        "    # compile network\r\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', \r\n",
        "                  metrics=['accuracy'])\r\n",
        "    # summarize defined model\r\n",
        "    model.summary()\r\n",
        "    return model\r\n",
        "\r\n",
        "model = create_model(vocabulary_size, max_length, embedding_matrix)\r\n",
        "\r\n",
        "# fit network\r\n",
        "model.fit(Xtrain, ytrain, epochs=20, verbose=2)\r\n",
        "# evaluate\r\n",
        "ytest = np.array(ytest)\r\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\r\n",
        "print('Test Accuracy: %f' % (acc*100))\r\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 1317, 100)         2576800   \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 1310, 32)          25632     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 655, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 20960)             0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                209610    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 2,812,053\n",
            "Trainable params: 235,253\n",
            "Non-trainable params: 2,576,800\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "57/57 - 10s - loss: 0.6839 - accuracy: 0.5400\n",
            "Epoch 2/20\n",
            "57/57 - 10s - loss: 0.6415 - accuracy: 0.6211\n",
            "Epoch 3/20\n",
            "57/57 - 10s - loss: 0.5303 - accuracy: 0.7644\n",
            "Epoch 4/20\n",
            "57/57 - 10s - loss: 0.3887 - accuracy: 0.8428\n",
            "Epoch 5/20\n",
            "57/57 - 10s - loss: 0.2802 - accuracy: 0.8928\n",
            "Epoch 6/20\n",
            "57/57 - 10s - loss: 0.1744 - accuracy: 0.9561\n",
            "Epoch 7/20\n",
            "57/57 - 10s - loss: 0.0897 - accuracy: 0.9894\n",
            "Epoch 8/20\n",
            "57/57 - 10s - loss: 0.0420 - accuracy: 0.9989\n",
            "Epoch 9/20\n",
            "57/57 - 12s - loss: 0.0238 - accuracy: 0.9994\n",
            "Epoch 10/20\n",
            "57/57 - 11s - loss: 0.0121 - accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "57/57 - 10s - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "57/57 - 10s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "57/57 - 10s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "57/57 - 10s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "57/57 - 10s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "57/57 - 10s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "57/57 - 10s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "57/57 - 10s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "57/57 - 10s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "57/57 - 10s - loss: 9.6339e-04 - accuracy: 1.0000\n",
            "Test Accuracy: 67.500001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yat7NrOK0uuW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}