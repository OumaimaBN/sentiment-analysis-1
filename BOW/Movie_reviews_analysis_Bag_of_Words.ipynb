{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie-reviews-analysis-Bag-of-Words",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLe9hHpU6vI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06235009-fa61-4adf-dccc-30f29f9694d8"
      },
      "source": [
        "import pandas as pd\r\n",
        "import string\r\n",
        "from string import punctuation\r\n",
        "from os import listdir\r\n",
        "from collections import Counter\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxEcVcgAzMvp",
        "outputId": "2bb6117e-8e70-4545-f1dd-20d324b2192e"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def load_doc(filename):\r\n",
        "\t# open the file as read only\r\n",
        "\tfile = open(filename, 'r')\r\n",
        "\t# read all text\r\n",
        "\ttext = file.read()\r\n",
        "\t# close the file\r\n",
        "\tfile.close()\r\n",
        "\treturn text\r\n",
        "# turn a doc into clean tokens\r\n",
        "\r\n",
        "def clean_doc(doc):\r\n",
        "\t# split into tokens by white space\r\n",
        "\ttokens = doc.split()\r\n",
        "\t# remove punctuation from each token\r\n",
        "\ttable = str.maketrans('', '', string.punctuation)\r\n",
        "\ttokens = [w.translate(table) for w in tokens]\r\n",
        "\t# remove remaining tokens that are not alphabetic\r\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\r\n",
        "\t# filter out stop words\r\n",
        "\tstop_words = set(stopwords.words('english'))\r\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\r\n",
        "\t# filter out short tokens\r\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\r\n",
        "\treturn tokens\r\n",
        "# %cd /content/drive/My Drive/\r\n",
        "%cd /content/drive/My Drive/TP1-NLP\r\n",
        "# load the document\r\n",
        "filename = './txt_sentoken/pos/cv000_29590.txt'\r\n",
        "text = load_doc(filename)\r\n",
        "tokens = clean_doc(text)\r\n",
        "print(tokens)\r\n",
        "def add_doc_to_vocab(filename, vocab):\r\n",
        "\t# load doc\r\n",
        "\tdoc = load_doc(filename)\r\n",
        "\t# clean doc\r\n",
        "\ttokens = clean_doc(doc)\r\n",
        "\t# update counts\r\n",
        "\tvocab.update(tokens)\r\n",
        "def process_docs(directory, vocab):\r\n",
        "\t# walk through all files in the folder\r\n",
        "\tfor filename in listdir(directory):\r\n",
        "\t\t# skip any reviews in the test set\r\n",
        "\t\tif filename.startswith('cv9'):\r\n",
        "\t\t\tcontinue\r\n",
        "\t\t# create the full path of the file to open\r\n",
        "\t\tpath = directory + '/' + filename\r\n",
        "\t\t# add doc to vocab\r\n",
        "\t\tload_doc(path)\r\n",
        "  \t# add doc to vocab\r\n",
        "\t\tadd_doc_to_vocab(path, vocab)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TP1-NLP\n",
            "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7z3j6UQ6eBv"
      },
      "source": [
        "# turn a doc into clean tokens\r\n",
        "def clean_doc(doc):\r\n",
        "\t# split into tokens by white space\r\n",
        "\ttokens = doc.split()\r\n",
        "\t# remove punctuation from each token\r\n",
        "\ttable = str.maketrans('', '', punctuation)\r\n",
        "\ttokens = [w.translate(table) for w in tokens]\r\n",
        "\t# remove remaining tokens that are not alphabetic\r\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\r\n",
        "\t# filter out stop words\r\n",
        "\tstop_words = set(stopwords.words('english'))\r\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\r\n",
        "\t# filter out short tokens\r\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\r\n",
        "\treturn tokens"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bqEXREy01Id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9c6d57-661b-43d9-c40f-631eaa87ca1f"
      },
      "source": [
        "# load doc and add to vocab\r\n",
        "\r\n",
        " \r\n",
        "# define vocab\r\n",
        "vocab = Counter()\r\n",
        "# add all docs to vocab\r\n",
        "process_docs('txt_sentoken/pos', vocab)\r\n",
        "process_docs('txt_sentoken/neg', vocab)\r\n",
        "# print the size of the vocab\r\n",
        "print(len(vocab))\r\n",
        "# print the top words in the vocab\r\n",
        "print(vocab.most_common(50))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('bad', 1248), ('could', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayoHLapx77UX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5171a775-cb18-47b7-f721-3cef0152cf46"
      },
      "source": [
        "# save list to file\r\n",
        "def save_list(lines, filename):\r\n",
        "\t# convert lines to a single blob of text\r\n",
        "\tdata = '\\n'.join(lines)\r\n",
        "\t# open file\r\n",
        "\tfile = open(filename, 'w')\r\n",
        "\t# write text\r\n",
        "\tfile.write(data)\r\n",
        "\t# close file\r\n",
        "\tfile.close()\r\n",
        "\r\n",
        "# keep tokens with a min occurrence\r\n",
        "min_occurane = 2\r\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\r\n",
        "print(len(tokens))\r\n",
        "# save tokens to a vocabulary file\r\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkHuPDP5mLY7"
      },
      "source": [
        "# load doc, clean and return line of tokens\r\n",
        "def doc_to_line(filename, vocab):\r\n",
        "\t# load the doc\r\n",
        "\tdoc = load_doc(filename)\r\n",
        "\t# clean doc\r\n",
        "\ttokens = clean_doc(doc)\r\n",
        "\t# filter by vocab\r\n",
        "\ttokens = [w for w in tokens if w in vocab]\r\n",
        "\treturn ' '.join(tokens)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa9W5CNZDPJc"
      },
      "source": [
        "# load all docs in a directory\r\n",
        "def process_docs(directory, vocab):\r\n",
        "\tlines = list()\r\n",
        "\t# walk through all files in the folder\r\n",
        "\tfor filename in listdir(directory):\r\n",
        "\t\t# skip any reviews in the test set\r\n",
        "\t\t'''if filename.startswith('cv9'):\r\n",
        "\t\t\tcontinue'''\r\n",
        "\t\t# create the full path of the file to open\r\n",
        "\t\tpath = directory + '/' + filename\r\n",
        "\t\t# load and clean the doc\r\n",
        "\t\tline = doc_to_line(path, vocab)\r\n",
        "\t\t# add to list\r\n",
        "\t\tlines.append(line)\r\n",
        "\treturn lines"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om4GRuSfDsQX"
      },
      "source": [
        "Finally, we need to load the vocabulary and turn it into a set for use in cleaning reviews.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssMs0X6UD5gY"
      },
      "source": [
        "from numpy import array\r\n",
        "def load_clean_dataset(vocab):\r\n",
        "  pos_labels = list()\r\n",
        "  neg_labels = list()\r\n",
        "  # load all training reviews\r\n",
        "  positive_lines = process_docs('txt_sentoken/pos', vocab)\r\n",
        "  for i in range(len(positive_lines)):\r\n",
        "    pos_labels.append(1)\r\n",
        "\r\n",
        "  negative_lines = process_docs('txt_sentoken/neg', vocab)\r\n",
        "  for i in range(len(negative_lines)):\r\n",
        "    neg_labels.append(0)\r\n",
        "  docs = positive_lines + negative_lines\r\n",
        "  labels = np.array(pos_labels + neg_labels)\r\n",
        "  return docs, labels\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3BZCILwEtyp"
      },
      "source": [
        "### Movie Reviews to Bag-of-Words Vectors\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNsGC_okEu6L"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\r\n",
        "# create the tokenizer\r\n",
        "def create_tokenizer(lines):\r\n",
        "  tokenizer = Tokenizer()\r\n",
        "  # fit the tokenizer on the documents\r\n",
        "  tokenizer.fit_on_texts(lines)\r\n",
        "  return tokenizer"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe0xjtXlM1Is",
        "outputId": "f8180ee6-88bc-4d13-ad7a-d7a9754b3112"
      },
      "source": [
        "# load the vocabulary\r\n",
        "vocab_filename = 'vocab.txt'\r\n",
        "vocab = load_doc(vocab_filename)\r\n",
        "vocab = set(vocab.split())\r\n",
        "# load all reviews\r\n",
        "docs, labels = load_clean_dataset(vocab)\r\n",
        "# create the tokenizer\r\n",
        "tokenizer = create_tokenizer(docs)\r\n",
        "# split data for training and testing\r\n",
        "split_limit = 1800\r\n",
        "train_docs, ytrain = docs[:split_limit], labels[:split_limit]\r\n",
        "test_docs, ytest = docs[split_limit:], labels[split_limit:]\r\n",
        "# encode data\r\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\r\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\r\n",
        "print(Xtrain.shape, Xtest.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1800, 25768) (200, 25768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NecwxfExZjM4"
      },
      "source": [
        "### Sentiment Analysis Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_gedIKsPB-d",
        "outputId": "ad5ab5e0-a905-462b-d40f-9d110a427ed6"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Dropout\r\n",
        "from numpy import array\r\n",
        "# define model\r\n",
        "def define_model(n_words):\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Dense(50, input_shape=(n_words,), activation='relu'))\r\n",
        "  model.add(Dense(1, activation='sigmoid'))\r\n",
        "  return model\r\n",
        "n_words = Xtest.shape[1]\r\n",
        "# compile network\r\n",
        "model = define_model(n_words)\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# fit network\r\n",
        "model.fit(Xtrain, ytrain, epochs=20, verbose=2)\r\n",
        "# evaluate\r\n",
        "#ytest = np.array(ytest)\r\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\r\n",
        "print('Test Accuracy: %f' % (acc*100))\r\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "57/57 - 1s - loss: 0.6902 - accuracy: 0.5483\n",
            "Epoch 2/20\n",
            "57/57 - 1s - loss: 0.6817 - accuracy: 0.5556\n",
            "Epoch 3/20\n",
            "57/57 - 1s - loss: 0.6671 - accuracy: 0.5556\n",
            "Epoch 4/20\n",
            "57/57 - 1s - loss: 0.6444 - accuracy: 0.5644\n",
            "Epoch 5/20\n",
            "57/57 - 1s - loss: 0.6143 - accuracy: 0.7244\n",
            "Epoch 6/20\n",
            "57/57 - 1s - loss: 0.5778 - accuracy: 0.8294\n",
            "Epoch 7/20\n",
            "57/57 - 1s - loss: 0.5382 - accuracy: 0.8906\n",
            "Epoch 8/20\n",
            "57/57 - 1s - loss: 0.4951 - accuracy: 0.8917\n",
            "Epoch 9/20\n",
            "57/57 - 1s - loss: 0.4556 - accuracy: 0.9472\n",
            "Epoch 10/20\n",
            "57/57 - 1s - loss: 0.4142 - accuracy: 0.9539\n",
            "Epoch 11/20\n",
            "57/57 - 1s - loss: 0.3771 - accuracy: 0.9561\n",
            "Epoch 12/20\n",
            "57/57 - 1s - loss: 0.3415 - accuracy: 0.9694\n",
            "Epoch 13/20\n",
            "57/57 - 1s - loss: 0.3097 - accuracy: 0.9711\n",
            "Epoch 14/20\n",
            "57/57 - 1s - loss: 0.2803 - accuracy: 0.9772\n",
            "Epoch 15/20\n",
            "57/57 - 1s - loss: 0.2543 - accuracy: 0.9828\n",
            "Epoch 16/20\n",
            "57/57 - 1s - loss: 0.2310 - accuracy: 0.9867\n",
            "Epoch 17/20\n",
            "57/57 - 1s - loss: 0.2092 - accuracy: 0.9889\n",
            "Epoch 18/20\n",
            "57/57 - 1s - loss: 0.1902 - accuracy: 0.9906\n",
            "Epoch 19/20\n",
            "57/57 - 1s - loss: 0.1740 - accuracy: 0.9894\n",
            "Epoch 20/20\n",
            "57/57 - 1s - loss: 0.1579 - accuracy: 0.9933\n",
            "Test Accuracy: 74.500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJape5Qj0Km5"
      },
      "source": [
        "### Making a Prediction for New Reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRdKOJcvamy2"
      },
      "source": [
        "def predict_sentiment(review, vocab, tokenizer, model):\r\n",
        "\t# clean\r\n",
        "\ttokens = clean_doc(review)\r\n",
        "\t# filter by vocab\r\n",
        "\ttokens = [w for w in tokens if w in vocab]\r\n",
        "\t# convert to line\r\n",
        "\tline = ' '.join(tokens)\r\n",
        "\t# encode\r\n",
        "\tencoded = tokenizer.texts_to_matrix([line], mode='freq')\r\n",
        "\t# prediction\r\n",
        "\tyhat = model.predict(encoded, verbose=0)\r\n",
        "\treturn round(yhat[0,0])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL4XbKn70Lwt",
        "outputId": "369a0ab8-0979-489b-9689-ed8bf2b74041"
      },
      "source": [
        "# test positive text\r\n",
        "text = 'Best movie ever!, i love it'\r\n",
        "print(predict_sentiment(text, vocab, tokenizer, model))\r\n",
        "# test negative text\r\n",
        "text = 'This is a bad movie.'\r\n",
        "print(predict_sentiment(text, vocab, tokenizer, model))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}